# =============================================================================
# NexusNitroLLM Environment Configuration
# =============================================================================
#
# Copy this file to .env and customize the values for your setup:
# cp env.example .env
#
# SECURITY WARNING: Never commit .env files with real credentials!

# =============================================================================
# SERVER CONFIGURATION
# =============================================================================

# Server settings
PORT=8080
HOST=0.0.0.0

# =============================================================================
# LLM BACKEND CONFIGURATION
# =============================================================================

# Primary backend URL - NexusNitroLLM auto-detects the backend type from URL:
# - LightLLM: http://localhost:8000
# - vLLM: http://localhost:8000/v1
# - OpenAI: https://api.openai.com/v1
# - Azure OpenAI: https://your-resource.openai.azure.com
# - Custom: https://your-endpoint.com/v1
nnLLM_URL=http://localhost:8000

# Backend type (usually auto-detected, but can override)
nnLLM_BACKEND_TYPE=lightllm

# Model to use
nnLLM_MODEL=llama

# Authentication token (if required by your backend)
# nnLLM_TOKEN=your_api_key_here

# =============================================================================
# UI CONFIGURATION (Optional)
# =============================================================================

# Admin interface credentials (if using web UI)
# UI_USERNAME=admin
# UI_PASSWORD=secure_password

# =============================================================================
# LITELLM PROXY CONFIGURATION (Optional)
# =============================================================================
# Only needed if using LiteLLM proxy as your backend

# LITELLM_BASE_URL=https://your-litellm-proxy.com
# LITELLM_ADMIN_TOKEN=your_admin_token
# LITELLM_VIRTUAL_KEY=your_virtual_key

# =============================================================================
# PERFORMANCE SETTINGS (Advanced)
# =============================================================================

# HTTP client settings
HTTP_CLIENT_TIMEOUT=30
HTTP_CLIENT_MAX_CONNECTIONS=100
HTTP_CLIENT_MAX_CONNECTIONS_PER_HOST=10

# Streaming settings
STREAMING_CHUNK_SIZE=1024
STREAMING_TIMEOUT=300
STREAMING_KEEP_ALIVE_INTERVAL=30

# =============================================================================
# FEATURE FLAGS
# =============================================================================

# Enable/disable features
ENABLE_STREAMING=true
ENABLE_BATCHING=false
ENABLE_RATE_LIMITING=true
ENABLE_CACHING=false
ENABLE_METRICS=true
ENABLE_HEALTH_CHECKS=true

# Force specific adapter (usually leave as "auto")
FORCE_ADAPTER=auto

# =============================================================================
# SECURITY & CORS
# =============================================================================

# CORS settings (for web frontends)
CORS_ORIGIN=*
CORS_METHODS=GET,POST,OPTIONS
CORS_HEADERS=*

# API key validation (if needed)
API_KEY_HEADER=X-API-Key
API_KEY_VALIDATION_ENABLED=false

# =============================================================================
# RATE LIMITING & CACHING
# =============================================================================

# Rate limiting
RATE_LIMIT_REQUESTS_PER_MINUTE=60
RATE_LIMIT_BURST_SIZE=10

# Response caching
CACHE_TTL_SECONDS=300
CACHE_MAX_SIZE=1000

# =============================================================================
# LOGGING & ENVIRONMENT
# =============================================================================

# Log level (error, warn, info, debug, trace)
RUST_LOG=info

# Show stack traces on errors (0, 1, or full)
# RUST_BACKTRACE=1

# Environment (development, staging, production)
ENVIRONMENT=development

# =============================================================================
# COMMON SETUPS
# =============================================================================

# Local LightLLM:
# nnLLM_URL=http://localhost:8000
# nnLLM_MODEL=llama
# nnLLM_TOKEN=

# OpenAI API:
# nnLLM_URL=https://api.openai.com/v1
# nnLLM_MODEL=gpt-3.5-turbo
# nnLLM_TOKEN=sk-your_openai_api_key

# Azure OpenAI:
# nnLLM_URL=https://your-resource.openai.azure.com
# nnLLM_MODEL=gpt-35-turbo
# nnLLM_TOKEN=your_azure_api_key