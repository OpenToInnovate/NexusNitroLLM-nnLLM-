/* auto-generated by NAPI-RS */
/* eslint-disable */
/**
 * Ultra-high-performance LightLLM client for Node.js
 *
 * This client provides direct access to Rust adapter code with zero HTTP overhead.
 * Optimized for maximum throughput in Node.js applications with proper async/await support.
 */
export declare class NodeNexusNitroLlmClient {
  /**
   * Create a new high-performance LightLLM client
   *
   * # Arguments
   * * `config` - Configuration object with connection details
   *
   * # Returns
   * * `Result<NodeNexusNitroLLMClient>` - New client instance or error
   *
   * # Performance Notes
   * * Uses connection pooling by default for maximum throughput
   * * Tokio runtime optimized for Node.js event loop integration
   * * Zero-copy data structures minimize garbage collection
   */
  constructor(config: NodeConfig)
  /**
   * Send chat completion request with maximum performance
   *
   * This method provides zero-overhead access to the Rust adapter by bypassing
   * HTTP serialization entirely. Perfect for high-throughput Node.js applications.
   *
   * # Arguments
   * * `request` - Chat completion request parameters
   *
   * # Returns
   * * `Promise<NodeChatResponse>` - Resolves to chat completion response
   *
   * # Performance
   * * Direct Rust function call (no HTTP overhead)
   * * Zero-copy message handling where possible
   * * Native async/await with proper Node.js event loop integration
   */
  chatCompletions(request: NodeChatRequest): Promise<NodeChatResponse>
  /**
   * Get performance statistics and configuration information
   *
   * Returns detailed information about the client's performance and configuration,
   * including whether it's running in direct mode or HTTP mode.
   *
   * # Returns
   * * `NodeStats` - Performance statistics and configuration
   */
  getStats(): NodeStats
  /**
   * Test connection to backend
   *
   * # Returns
   * * `Promise<bool>` - Resolves to true if connection successful, false otherwise
   */
  testConnection(): Promise<boolean>
  /**
   * Update configuration dynamically
   *
   * # Arguments
   * * `new_config` - New configuration to apply
   *
   * # Performance Notes
   * * Configuration changes are applied immediately
   * * Connection pool is recreated if connection settings change
   */
  updateConfig(newConfig: NodeConfig): void
}
export type NodeNexusNitroLLMClient = NodeNexusNitroLlmClient

/**
 * Run performance benchmark
 *
 * # Arguments
 * * `client` - Client to benchmark
 * * `operations` - Number of operations to perform
 *
 * # Returns
 * * `NodeBenchmark` - Performance statistics
 */
export declare function benchmarkClient(client: NodeNexusNitroLlmClient, operations: number): NodeBenchmark

/**
 * Create a high-performance LightLLM client (convenience function)
 *
 * # Arguments
 * * `backend_url` - Backend server URL
 * * `model_id` - Default model identifier
 * * `options` - Optional configuration parameters
 *
 * # Returns
 * * `Promise<NodeNexusNitroLLMClient>` - High-performance client instance
 */
export declare function createClient(backendUrl: string, backendType: string | undefined | null, modelId: string, options?: NodeConfig | undefined | null): NodeNexusNitroLlmClient

/**
 * Create a high-performance configuration object
 *
 * # Arguments
 * * `backend_url` - Backend server URL
 * * `model_id` - Default model identifier
 * * `options` - Optional configuration parameters
 *
 * # Returns
 * * `NodeConfig` - Optimized configuration object
 */
export declare function createConfig(backendUrl: string, backendType: string | undefined | null, modelId: string, options?: NodeConfig | undefined | null): NodeConfig

/**
 * Get library version information
 * Create a direct mode client for maximum performance
 *
 * This creates a client that bypasses HTTP entirely for maximum performance.
 * Perfect for Node.js applications that want direct integration without network overhead.
 *
 * # Arguments
 * * `model_id` - Model identifier (default: "llama")
 * * `token` - Optional authentication token
 *
 * # Returns
 * * `Result<NodeNexusNitroLLMClient>` - New direct mode client or error
 *
 * # Performance Benefits
 * * Zero HTTP overhead
 * * Direct memory access
 * * Minimal latency
 * * Maximum throughput
 */
export declare function createDirectClient(modelId?: string | undefined | null, token?: string | undefined | null): NodeNexusNitroLlmClient

/**
 * Create an HTTP mode client for traditional proxy communication
 *
 * This creates a client that communicates with a LightLLM server via HTTP.
 * Use this when you need to share the backend across multiple applications.
 *
 * # Arguments
 * * `backend_url` - Backend server URL
 * * `model_id` - Model identifier (default: "llama")
 * * `token` - Optional authentication token
 *
 * # Returns
 * * `Result<NodeNexusNitroLLMClient>` - New HTTP mode client or error
 */
export declare function createHttpClient(backendUrl: string, backendType?: string | undefined | null, modelId?: string | undefined | null, token?: string | undefined | null): NodeNexusNitroLlmClient

/**
 * Create an optimized message object
 *
 * # Arguments
 * * `role` - Message role ("system", "user", "assistant", "tool")
 * * `content` - Message content
 * * `name` - Optional message name
 *
 * # Returns
 * * `NodeMessage` - Zero-copy optimized message
 */
export declare function createMessage(role: string, content: string, name?: string | undefined | null): NodeMessage

export declare function getVersion(): string

/** Get performance benchmarking utilities */
export interface NodeBenchmark {
  /** Operations per second */
  opsPerSecond: number
  /** Average latency in milliseconds */
  avgLatencyMs: number
  /** Memory usage in MB */
  memoryMb: number
}

/** Chat completion request parameters for Node.js */
export interface NodeChatRequest {
  /** List of messages in the conversation */
  messages: Array<NodeMessage>
  /** Model to use (optional, uses config default if not specified) */
  model?: string
  /** Maximum tokens to generate */
  maxTokens?: number
  /** Sampling temperature (0.0 to 2.0) */
  temperature?: number
  /** Nucleus sampling parameter */
  topP?: number
  /** Number of completions to generate */
  n?: number
  /** Whether to stream the response */
  stream?: boolean
  /** Stop sequences */
  stop?: Array<string>
  /** Presence penalty (-2.0 to 2.0) */
  presencePenalty?: number
  /** Frequency penalty (-2.0 to 2.0) */
  frequencyPenalty?: number
  /** User identifier for tracking */
  user?: string
}

/** Chat completion response for Node.js */
export interface NodeChatResponse {
  /** Unique response identifier */
  id: string
  /** Object type ("chat.completion") */
  object: string
  /** Creation timestamp */
  created: number
  /** Model used */
  model: string
  /** Response choices */
  choices: Array<NodeChoice>
  /** Token usage information */
  usage: NodeUsage
}

/** Individual choice in response */
export interface NodeChoice {
  /** Choice index */
  index: number
  /** Response message */
  message: NodeMessage
  /** Finish reason */
  finishReason: string
}

/**
 * High-performance configuration for Node.js applications
 *
 * Optimized for maximum throughput and minimal latency in Node.js environments.
 * All configuration changes are applied immediately for real-time performance tuning.
 *
 * ## Direct Mode vs HTTP Mode
 * - **Direct Mode**: Set `backend_url` to `null` or `"direct"` for maximum performance
 * - **HTTP Mode**: Provide a valid URL for traditional proxy communication
 */
export interface NodeConfig {
  /** Backend LLM server URL (null or "direct" for direct mode) */
  backendUrl?: string
  /** Backend LLM type (lightllm, vllm, openai, azure, aws, etc.) */
  backendType?: string
  /** Default model identifier */
  modelId: string
  /** Server port (optional) */
  port?: number
  /** Authentication token (optional) */
  token?: string
  /** Enable connection pooling for maximum performance */
  connectionPooling?: boolean
  /** Maximum HTTP connections (performance tuning) */
  maxConnections?: number
  /** Maximum connections per host (performance tuning) */
  maxConnectionsPerHost?: number
}

/**
 * High-performance message structure for Node.js
 *
 * Designed for zero-copy operations and minimal garbage collection pressure.
 */
export interface NodeMessage {
  /** Message role: "system", "user", "assistant", or "tool" */
  role: string
  /** Message content */
  content: string
  /** Optional message name */
  name?: string
}

/** Statistics for performance monitoring */
export interface NodeStats {
  /** Adapter type being used (lightllm, openai, or direct) */
  adapterType: string
  /** Backend URL (or "direct" for direct mode) */
  backendUrl: string
  /** Model ID being used */
  modelId: string
  /** Server port (if applicable) */
  port?: number
  /** Whether connection pooling is enabled */
  connectionPooling: boolean
  /** Maximum connections configured */
  maxConnections: number
  /** Maximum connections per host */
  maxConnectionsPerHost: number
  /** Timeout in seconds */
  timeoutSeconds: number
  /** Whether running in direct mode (no HTTP overhead) */
  isDirectMode: boolean
  /** Performance mode description */
  performanceMode: string
}

/** Token usage statistics */
export interface NodeUsage {
  /** Tokens in prompt */
  promptTokens: number
  /** Tokens in completion */
  completionTokens: number
  /** Total tokens used */
  totalTokens: number
}
